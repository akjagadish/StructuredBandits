{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we directly compare two models, GP-RBF/Clustering hybrid and the GP-RBF/Kalman hybrid, in the change point experiment as the LOO scores for both models were extremly close. As such, we want to calculate the standard error of the difference score to test whether they are statistically distinguishable. This requires re-estimating both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runing on PyMC3 v3.4.1\n",
      "Runing on Theano v1.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/gp_learning/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pymc3 as pm\n",
    "from theano.tensor.nnet.nnet import softmax\n",
    "from theano import tensor as tt\n",
    "import theano as t\n",
    "\n",
    "print('Runing on PyMC3 v{}'.format(pm.__version__))\n",
    "print('Runing on Theano v{}'.format(t.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sticky_choice(raw_data, n_arms=8):\n",
    "    x_sc = []\n",
    "    for subj in set(raw_data['id']):\n",
    "        y = pd.get_dummies(raw_data.loc[raw_data['id'] == subj, 'arm'])\n",
    "\n",
    "        # so, not every one uses every response, so we need to correct for this\n",
    "        for c in set(range(1, n_arms + 1)):\n",
    "            if c not in set(y.columns):\n",
    "                y[c] = np.zeros(len(y), dtype=int)\n",
    "        y = y.values\n",
    "\n",
    "        x_sc.append(np.concatenate([np.zeros((1, n_arms)), y[:-1, :]]))\n",
    "\n",
    "    return np.concatenate(x_sc)\n",
    "\n",
    "\n",
    "def construct_subj_idx(data_frame):\n",
    "    subj_idx = []\n",
    "    subjs_inc = {}\n",
    "    for s in data_frame['id'].values:\n",
    "        if s not in subjs_inc:\n",
    "            subjs_inc[s] = len(subjs_inc)\n",
    "        subj_idx.append(subjs_inc[s])\n",
    "    subj_idx = np.array(subj_idx)\n",
    "    return subj_idx\n",
    "\n",
    "\n",
    "clustering_data = pd.read_pickle('Data/exp_changepoint/exp_cp_clustering_means_std.pkl')\n",
    "clustering_data.index = range(len(clustering_data))\n",
    "\n",
    "lin_gp_data = pd.read_csv('Data/exp_changepoint/changelinpred.csv')\n",
    "lin_gp_data.index = range(len(lin_gp_data))\n",
    "\n",
    "rbf_gp_data = pd.read_csv('Data/exp_changepoint/changerbfpred.csv')\n",
    "rbf_gp_data.index = range(len(rbf_gp_data))\n",
    "\n",
    "kalman_data = pd.read_csv('Data/exp_changepoint/changekalmanpred.csv')\n",
    "kalman_data.index = range(len(kalman_data))\n",
    "\n",
    "bayes_gp_data = pd.read_pickle('Data/exp_changepoint/bayes_gp_exp_cp.pkl')\n",
    "bayes_gp_data.index = range(len(bayes_gp_data))\n",
    "\n",
    "raw_data = pd.read_csv('Data/exp_changepoint/changepoint.csv', header=0)\n",
    "\n",
    "# the GP-RBF can fail if subject always choose the same response. For simplicity, we are dropping those\n",
    "# subjects\n",
    "subjects_to_drop = set()\n",
    "for s in set(raw_data.id):\n",
    "    if s not in set(rbf_gp_data.id):\n",
    "        subjects_to_drop.add(s)\n",
    "\n",
    "for s in subjects_to_drop:\n",
    "    clustering_data = clustering_data[clustering_data['Subject'] != s].copy()\n",
    "    lin_gp_data = lin_gp_data[lin_gp_data.id != s].copy()\n",
    "    raw_data = raw_data[raw_data.id != s].copy()\n",
    "    kalman_data = kalman_data[kalman_data.id != s].copy()\n",
    "    bayes_gp_data = bayes_gp_data[bayes_gp_data['Subject'] != s].copy()\n",
    "\n",
    "# construct a sticky choice predictor. This is the same for all of the models\n",
    "x_sc = construct_sticky_choice(raw_data)\n",
    "\n",
    "# PYMC3 doesn't care about the actual subject numbers, so remap these to a sequential list\n",
    "subj_idx = construct_subj_idx(lin_gp_data)\n",
    "n_subj = len(set(subj_idx))\n",
    "\n",
    "# prep the predictor vectors\n",
    "x_mu_cls = np.array([clustering_data.loc[:, 'mu_%d' % ii].values for ii in range(8)]).T\n",
    "x_sd_cls = np.array([clustering_data.loc[:, 'std_%d' % ii].values for ii in range(8)]).T\n",
    "\n",
    "x_mu_bayes_gp = np.array([bayes_gp_data.loc[:, 'mu_%d' % ii].values for ii in range(8)]).T\n",
    "x_sd_bayes_gp = np.array([bayes_gp_data.loc[:, 'std_%d' % ii].values for ii in range(8)]).T\n",
    "\n",
    "x_mu_lin = np.array([lin_gp_data.loc[:, 'mu_%d' % ii].values for ii in range(8)]).T\n",
    "x_sd_lin = np.array([lin_gp_data.loc[:, 'sigma_%d' % ii].values for ii in range(8)]).T\n",
    "\n",
    "x_mu_rbf = np.array([rbf_gp_data.loc[:, 'mu_%d' % ii].values for ii in range(8)]).T\n",
    "x_sd_rbf = np.array([rbf_gp_data.loc[:, 'sigma_%d' % ii].values for ii in range(8)]).T\n",
    "\n",
    "x_mu_kal = np.array([kalman_data.loc[:, 'mu_%d' % ii].values for ii in range(8)]).T\n",
    "x_sd_kal = np.array([kalman_data.loc[:, 'sig_%d' % ii].values for ii in range(8)]).T\n",
    "\n",
    "y = raw_data['arm'].values - 1  # convert to 0 indexing\n",
    "\n",
    "\n",
    "# print \"Experiment Change Point, Running %d subjects\" % model_matrix['n_subj']\n",
    "# run_save_models(model_matrix, name_tag='exp_cp', sample_kwargs=sample_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate GP-RBF/Clustering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using advi+adapt_diag...\n",
      "Average Loss = 41,183:  20%|██        | 40697/200000 [18:43<1:13:17, 36.23it/s]   \n",
      "Convergence archived at 40700\n",
      "Interrupted at 40,699 [20%]: Average Loss = 51,518\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [beta_sc, beta_cls_std, beta_cls_mu, beta_rbf_std, beta_rbf_mu, sigma_stick_log__, sigma_cls_stdev_log__, sigma_cls_means_log__, sigma_rbf_stdev_log__, sigma_rbf_means_log__, mu_beta_stick, mu_beta_cls_stdv, mu_beta_cls_mean, mu_beta_rbf_stdv, mu_beta_rbf_mean]\n"
     ]
    }
   ],
   "source": [
    "n, d = x_mu_cls.shape\n",
    "sample_kwargs = dict(draws=2000, njobs=2, tune=2000, init='advi+adapt_diag')\n",
    "\n",
    "with pm.Model() as hier_rbf_clus:\n",
    "    mu_1 = pm.Normal('mu_beta_rbf_mean', mu=0., sd=100.)\n",
    "    mu_2 = pm.Normal('mu_beta_rbf_stdv', mu=0., sd=100.)\n",
    "    mu_3 = pm.Normal('mu_beta_cls_mean', mu=0., sd=100.)\n",
    "    mu_4 = pm.Normal('mu_beta_cls_stdv', mu=0., sd=100.)\n",
    "    mu_5 = pm.Normal('mu_beta_stick',    mu=0., sd=100.)\n",
    "\n",
    "    sigma_1 = pm.HalfCauchy('sigma_rbf_means', beta=100)\n",
    "    sigma_2 = pm.HalfCauchy('sigma_rbf_stdev', beta=100)\n",
    "    sigma_3 = pm.HalfCauchy('sigma_cls_means', beta=100)\n",
    "    sigma_4 = pm.HalfCauchy('sigma_cls_stdev', beta=100)\n",
    "    sigma_5 = pm.HalfCauchy('sigma_stick',     beta=100)\n",
    "\n",
    "    b_1 = pm.Normal('beta_rbf_mu',  mu=mu_1, sd=sigma_1, shape=n_subj)\n",
    "    b_2 = pm.Normal('beta_rbf_std', mu=mu_2, sd=sigma_2, shape=n_subj)\n",
    "    b_3 = pm.Normal('beta_cls_mu',  mu=mu_3, sd=sigma_3, shape=n_subj)\n",
    "    b_4 = pm.Normal('beta_cls_std', mu=mu_4, sd=sigma_4, shape=n_subj)\n",
    "    b_5 = pm.Normal('beta_sc',      mu=mu_5, sd=sigma_5, shape=n_subj)\n",
    "\n",
    "    rho = \\\n",
    "        tt.tile(tt.reshape(b_1[subj_idx], (n, 1)), d) * x_mu_rbf + \\\n",
    "        tt.tile(tt.reshape(b_2[subj_idx], (n, 1)), d) * x_sd_rbf + \\\n",
    "        tt.tile(tt.reshape(b_3[subj_idx], (n, 1)), d) * x_mu_cls + \\\n",
    "        tt.tile(tt.reshape(b_4[subj_idx], (n, 1)), d) * x_sd_cls + \\\n",
    "        tt.tile(tt.reshape(b_5[subj_idx], (n, 1)), d) * x_sc\n",
    "\n",
    "    p_hat = softmax(rho)\n",
    "\n",
    "    # Data likelihood\n",
    "    yl = pm.Categorical('yl', p=p_hat, observed=y)\n",
    "\n",
    "    # inference! (note: the progress bar may show up in the terminal window)\n",
    "    trace_gprbf_cls = pm.sample(**sample_kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate GP-RBF/Kalman Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using advi+adapt_diag...\n",
      "Average Loss = 41,182:  21%|██▏       | 42596/200000 [24:39<1:31:08, 28.79it/s]   \n",
      "Convergence archived at 42600\n",
      "Interrupted at 42,599 [21%]: Average Loss = 51,093\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [beta_sc, beta_cls_std, beta_cls_mu, beta_lin_std, beta_lin_mu, sigma_stick_log__, sigma_cls_stdev_log__, sigma_cls_means_log__, sigma_lin_stdev_log__, sigma_lin_means_log__, mu_beta_stick, mu_beta_cls_stdv, mu_beta_cls_mean, mu_beta_lin_stdv, mu_beta_lin_mean]\n"
     ]
    }
   ],
   "source": [
    "with pm.Model() as hier_lin_clus:\n",
    "    mu_1 = pm.Normal('mu_beta_lin_mean', mu=0., sd=100.)\n",
    "    mu_2 = pm.Normal('mu_beta_lin_stdv', mu=0., sd=100.)\n",
    "    mu_3 = pm.Normal('mu_beta_cls_mean', mu=0., sd=100.)\n",
    "    mu_4 = pm.Normal('mu_beta_cls_stdv', mu=0., sd=100.)\n",
    "    mu_5 = pm.Normal('mu_beta_stick',    mu=0., sd=100.)\n",
    "\n",
    "    sigma_1 = pm.HalfCauchy('sigma_lin_means', beta=100)\n",
    "    sigma_2 = pm.HalfCauchy('sigma_lin_stdev', beta=100)\n",
    "    sigma_3 = pm.HalfCauchy('sigma_cls_means', beta=100)\n",
    "    sigma_4 = pm.HalfCauchy('sigma_cls_stdev', beta=100)\n",
    "    sigma_5 = pm.HalfCauchy('sigma_stick',     beta=100)\n",
    "\n",
    "    b_1 = pm.Normal('beta_lin_mu',  mu=mu_1, sd=sigma_1, shape=n_subj)\n",
    "    b_2 = pm.Normal('beta_lin_std', mu=mu_2, sd=sigma_2, shape=n_subj)\n",
    "    b_3 = pm.Normal('beta_cls_mu',  mu=mu_3, sd=sigma_3, shape=n_subj)\n",
    "    b_4 = pm.Normal('beta_cls_std', mu=mu_4, sd=sigma_4, shape=n_subj)\n",
    "    b_5 = pm.Normal('beta_sc',      mu=mu_5, sd=sigma_5, shape=n_subj)\n",
    "\n",
    "    rho = \\\n",
    "        tt.tile(tt.reshape(b_1[subj_idx], (n, 1)), d) * x_mu_lin + \\\n",
    "        tt.tile(tt.reshape(b_2[subj_idx], (n, 1)), d) * x_sd_lin + \\\n",
    "        tt.tile(tt.reshape(b_3[subj_idx], (n, 1)), d) * x_mu_cls + \\\n",
    "        tt.tile(tt.reshape(b_4[subj_idx], (n, 1)), d) * x_sd_cls + \\\n",
    "        tt.tile(tt.reshape(b_5[subj_idx], (n, 1)), d) * x_sc\n",
    "\n",
    "    p_hat = softmax(rho)\n",
    "\n",
    "    # Data likelihood\n",
    "    yl = pm.Categorical('yl', p=p_hat, observed=y)\n",
    "\n",
    "    # inference!\n",
    "    trace_gplin_cls = pm.sample(**sample_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directly Compare the two models using the Standard error of the LOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/gp_learning/lib/python2.7/site-packages/pymc3/stats.py:292: UserWarning: Estimated shape parameter of Pareto distribution is\n",
      "        greater than 0.7 for one or more samples.\n",
      "        You should consider using a more robust model, this is because\n",
      "        importance sampling is less likely to work well if the marginal\n",
      "        posterior and LOO posterior are very different. This is more likely to\n",
      "        happen with a non-robust model and highly influential observations.\n",
      "  happen with a non-robust model and highly influential observations.\"\"\")\n"
     ]
    }
   ],
   "source": [
    "df_comp_loo = pm.compare({hier_rbf_clus:trace_gprbf_cls, hier_lin_clus: trace_gplin_cls}, ic='LOO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOO</th>\n",
       "      <th>pLOO</th>\n",
       "      <th>dLOO</th>\n",
       "      <th>weight</th>\n",
       "      <th>SE</th>\n",
       "      <th>dSE</th>\n",
       "      <th>shape_warn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81324.1</td>\n",
       "      <td>395.66</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>479.89</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81325.8</td>\n",
       "      <td>396.37</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0</td>\n",
       "      <td>479.86</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       LOO    pLOO  dLOO weight      SE   dSE shape_warn\n",
       "1  81324.1  395.66     0      1  479.89     0          1\n",
       "0  81325.8  396.37  1.76      0  479.86  0.82          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comp_loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2695716359822877"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log2(np.exp(df_comp_loo['dLOO'].max() / -2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
